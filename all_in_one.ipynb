{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Running\n\nThis notebook can be run from the Kaggle environment. It should take around 15 hours in total. This version is combined from three separate notebooks. Those three notebooks run perfectly. This notebook has not been run before due to the Kaggle time limit."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nfrom PIL import Image\nimport argparse\nimport torch\nimport torch.utils.data\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image\nimport torchvision.models as models\nimport matplotlib\nimport matplotlib.image as image\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Bidirectional, Embedding\nfrom keras.callbacks import ModelCheckpoint","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"file_path = '/kaggle/input/uw-cs480-fall20/'\nimage_path = '/kaggle/input/uw-cs480-fall20/suffled-images/shuffled-images/'\n\ndef load_data():\n    train_df = pd.read_csv(file_path + 'train.csv')\n    test_df = pd.read_csv(file_path + 'test.csv')\n    return train_df, test_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df, test_df = load_data()\ndata_size = len(train_df)\nprint(data_size)\n\n# remove all free gifts\ntrain_df = train_df[train_df.category != 'Free Gifts']\ndata_size = len(train_df)\nprint(data_size)\n\ncategories = train_df.category.unique()\ncategory_d = {k: v for v, k in enumerate(categories)}\n\ngenders = train_df.gender.unique()\ngender_d = {k: v for v, k in enumerate(genders)}\nnum_genders = len(genders)\n\nbaseColours = train_df.baseColour.unique()\nbaseColour_d = {k: v for v, k in enumerate(baseColours)}\nnum_baseColours = len(baseColours)\n\nseasons = train_df.season.unique()\nseason_d = {k: v for v, k in enumerate(seasons)}\nnum_seasons = len(seasons)\n\nusages = train_df.usage.unique()\nusage_d = {k: v for v, k in enumerate(usages)}\nnum_usages = len(usages)\n\n# training data\n\ntrain_df.replace(\n    {'category': category_d,\n     'gender': gender_d,\n     'baseColour': baseColour_d,\n     'season': season_d,\n     'usage': usage_d}\n    , inplace=True\n)\n\n# testing data\n\ntest_df.replace(\n    {'category': category_d,\n     'gender': gender_d,\n     'baseColour': baseColour_d,\n     'season': season_d,\n     'usage': usage_d}\n    , inplace=True\n)\n\n\npreprocess_training = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\npreprocess_test = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nclass Image_Dataset(Dataset):\n\n    def __init__(self, id_target, folder=image_path, transform=preprocess_training):\n        self.id_target = id_target\n        self.folder = folder\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.id_target)\n\n    def __getitem__(self, idx):\n        img_name = self.folder + str(id_target[idx][0]) + '.jpg'\n        image = Image.open(img_name)\n        result = self.transform(image)\n\n        return result, id_target[idx][1]\n\n\nid_target = train_df[['id', 'category']].values\n\nsplits = np.array_split(id_target, 5)\n\ntrain_data1 = np.concatenate(np.delete(splits, 4, 0))\nvalidation_data1 = splits[4]\n\ntrain_data2 = np.concatenate(np.delete(splits, 3, 0))\nvalidation_data2 = splits[3]\n\ntrain_data3 = np.concatenate(np.delete(splits, 2, 0))\nvalidation_data3 = splits[2]\n\ntrain_data4 = np.concatenate(np.delete(splits, 1, 0))\nvalidation_data4 = splits[1]\n\ntrain_data5 = np.concatenate(np.delete(splits, 0, 0))\nvalidation_data5 = splits[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64\nlog_interval = 100\n\n# run on GPU if possible\ncuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if cuda else \"cpu\")\n\n# create data loaders\nkwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n\ntrain1 = Image_Dataset(train_data1)\nvalidation1 = Image_Dataset(validation_data1, transform=preprocess_test)\n\ntrain2 = Image_Dataset(train_data2)\nvalidation2 = Image_Dataset(validation_data2, transform=preprocess_test)\n\ntrain3 = Image_Dataset(train_data3)\nvalidation3 = Image_Dataset(validation_data3, transform=preprocess_test)\n\ntrain4 = Image_Dataset(train_data4)\nvalidation4 = Image_Dataset(validation_data4, transform=preprocess_test)\n\ntrain5 = Image_Dataset(train_data5)\nvalidation5 = Image_Dataset(validation_data5, transform=preprocess_test)\n\n\ntrain_loader1 = DataLoader(train1, batch_size=batch_size, shuffle=True, **kwargs)\nvalidation_loader1 = DataLoader(validation1, batch_size=batch_size, shuffle=True, **kwargs)\n\ntrain_loader2 = DataLoader(train2, batch_size=batch_size, shuffle=True, **kwargs)\nvalidation_loader2 = DataLoader(validation2, batch_size=batch_size, shuffle=True, **kwargs)\n\ntrain_loader3 = DataLoader(train3, batch_size=batch_size, shuffle=True, **kwargs)\nvalidation_loader3 = DataLoader(validation3, batch_size=batch_size, shuffle=True, **kwargs)\n\ntrain_loader4 = DataLoader(train4, batch_size=batch_size, shuffle=True, **kwargs)\nvalidation_loader4 = DataLoader(validation4, batch_size=batch_size, shuffle=True, **kwargs)\n\ntrain_loader5 = DataLoader(train5, batch_size=batch_size, shuffle=True, **kwargs)\nvalidation_loader5 = DataLoader(validation5, batch_size=batch_size, shuffle=True, **kwargs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"num_classes = 26\n\nassert(len(categories) == num_classes)\n\nmodel_image1 = models.resnet50(num_classes=num_classes).to(device)\nmodel_image2 = models.resnet50(num_classes=num_classes).to(device)\nmodel_image3 = models.resnet50(num_classes=num_classes).to(device)\nmodel_image4 = models.resnet50(num_classes=num_classes).to(device)\nmodel_image5 = models.resnet50(num_classes=num_classes).to(device)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loss Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion_image1 = nn.CrossEntropyLoss(reduction='sum')\ncriterion_image2 = nn.CrossEntropyLoss(reduction='sum')\ncriterion_image3 = nn.CrossEntropyLoss(reduction='sum')\ncriterion_image4 = nn.CrossEntropyLoss(reduction='sum')\ncriterion_image5 = nn.CrossEntropyLoss(reduction='sum')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Optimizers"},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer_image1 = optim.Adam(model_image1.parameters(), lr=1e-3)\noptimizer_image2 = optim.Adam(model_image2.parameters(), lr=1e-3)\noptimizer_image3 = optim.Adam(model_image3.parameters(), lr=1e-3)\noptimizer_image4 = optim.Adam(model_image4.parameters(), lr=1e-3)\noptimizer_image5 = optim.Adam(model_image5.parameters(), lr=1e-3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def image_train(epoch, model, optimizer, criterion_image, train_loader):\n    model.train()\n    total_loss = 0\n    for batch_idx, (images, targets) in enumerate(train_loader):\n        \n        images = images.to(device)\n        targets = targets.to(device)\n        optimizer.zero_grad()\n        recon_batch = model(images)\n        \n        loss = criterion_image(recon_batch, targets)\n        loss.backward()\n        total_loss += loss.item()\n        optimizer.step()\n        if batch_idx % log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(images), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader),\n                loss.item() / len(images)))\n\n    average_loss = total_loss / len(train_loader.dataset)\n    print('====> Epoch: {} Average loss: {:.4f}'.format(\n          epoch, average_loss))\n    return average_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def image_test(epoch, model, criterion_image, validation_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for _, (images, targets) in enumerate(validation_loader):\n            images = images.to(device)\n            targets = targets.to(device)\n            recon_batch = model(images)\n            test_loss += criterion_image(recon_batch, targets).item()\n            \n            preds = recon_batch.argmax(dim=1)#, keepdim=True)\n            correct += preds.eq(targets).sum().item()\n            \n    average_test_loss = test_loss / len(validation_loader.dataset)\n    test_accuracy = correct / len(validation_loader.dataset)\n    print('====> Validation loss: {:.4f}'.format(average_test_loss))\n    print('====> Validation accuracy: {:.2f}'.format(test_accuracy))\n    return average_test_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Image Main"},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Image Model1"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train and test\n\naverage_train_losses = []\naverage_test_losses = []\n\nfor epoch in range(1, epochs + 1):\n    average_train_loss = image_train(epoch, model_image1, optimizer_image1, criterion_image1, train_loader1)\n    average_train_losses.append(average_train_loss)\n    average_test_loss = image_test(epoch, model_image1, criterion_image1, validation_loader1)\n\n    # save model with best validation loss\n    if epoch == 1 or average_test_loss < min(average_test_losses):\n        torch.save(model_image1, 'image_classification_augmentation_model1.pt')\n\n    average_test_losses.append(average_test_loss)\n    \n\n# Plot Training Losses\nplt.plot(average_train_losses)\nplt.title('Train Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Train'], loc='upper right')\nplt.show()\n\n# Plot Testing Losses\nplt.plot(average_test_losses)\nplt.title('Test Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Test'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Image Model2"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train and test\n\naverage_train_losses = []\naverage_test_losses = []\n\nfor epoch in range(1, epochs + 1):\n    average_train_loss = image_train(epoch, model_image2, optimizer_image2, criterion_image2, train_loader2)\n    average_train_losses.append(average_train_loss)\n    average_test_loss = image_test(epoch, model_image2, criterion_image2, validation_loader2)\n\n    # save model with best validation loss\n    if epoch == 1 or average_test_loss < min(average_test_losses):\n        torch.save(model_image2, 'image_classification_augmentation_model2.pt')\n\n    average_test_losses.append(average_test_loss)\n    \n\n# Plot Training Losses\nplt.plot(average_train_losses)\nplt.title('Train Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Train'], loc='upper right')\nplt.show()\n\n# Plot Testing Losses\nplt.plot(average_test_losses)\nplt.title('Test Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Test'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Image Model3"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train and test\n\naverage_train_losses = []\naverage_test_losses = []\n\nfor epoch in range(1, epochs + 1):\n    average_train_loss = image_train(epoch, model_image3, optimizer_image3, criterion_image3, train_loader3)\n    average_train_losses.append(average_train_loss)\n    average_test_loss = image_test(epoch, model_image3, criterion_image3, validation_loader3)\n\n    # save model with best validation loss\n    if epoch == 1 or average_test_loss < min(average_test_losses):\n        torch.save(model_image3, 'image_classification_augmentation_model3.pt')\n\n    average_test_losses.append(average_test_loss)\n    \n\n# Plot Training Losses\nplt.plot(average_train_losses)\nplt.title('Train Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Train'], loc='upper right')\nplt.show()\n\n# Plot Testing Losses\nplt.plot(average_test_losses)\nplt.title('Test Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Test'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Image Model4"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train and test\n\naverage_train_losses = []\naverage_test_losses = []\n\nfor epoch in range(1, epochs + 1):\n    average_train_loss = image_train(epoch, model_image4, optimizer_image4, criterion_image4, train_loader4)\n    average_train_losses.append(average_train_loss)\n    average_test_loss = image_test(epoch, model_image4, criterion_image4, validation_loader4)\n\n    # save model with best validation loss\n    if epoch == 1 or average_test_loss < min(average_test_losses):\n        torch.save(model_image4, 'image_classification_augmentation_model4.pt')\n\n    average_test_losses.append(average_test_loss)\n    \n\n# Plot Training Losses\nplt.plot(average_train_losses)\nplt.title('Train Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Train'], loc='upper right')\nplt.show()\n\n# Plot Testing Losses\nplt.plot(average_test_losses)\nplt.title('Test Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Test'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Image Model5"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train and test\n\naverage_train_losses = []\naverage_test_losses = []\n\nfor epoch in range(1, epochs + 1):\n    average_train_loss = image_train(epoch, model_image5, optimizer_image5, criterion_image5, train_loader5)\n    average_train_losses.append(average_train_loss)\n    average_test_loss = image_test(epoch, model_image5, criterion_image5, validation_loader5)\n\n    # save model with best validation loss\n    if epoch == 1 or average_test_loss < min(average_test_losses):\n        torch.save(model_image5, 'image_classification_augmentation_model5.pt')\n\n    average_test_losses.append(average_test_loss)\n    \n\n# Plot Training Losses\nplt.plot(average_train_losses)\nplt.title('Train Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Train'], loc='upper right')\nplt.show()\n\n# Plot Testing Losses\nplt.plot(average_test_losses)\nplt.title('Test Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Test'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save Image Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_path = ''\n\nmodel1 = torch.load(model_path + 'image_classification_augmentation_model1.pt')\nmodel1 = model1.to(device)\nmodel1.eval()\n\nmodel2 = torch.load(model_path + 'image_classification_augmentation_model2.pt')\nmodel2 = model2.to(device)\nmodel2.eval()\n\nmodel3 = torch.load(model_path + 'image_classification_augmentation_model3.pt')\nmodel3 = model3.to(device)\nmodel3.eval()\n\nmodel4 = torch.load(model_path + 'image_classification_augmentation_model4.pt')\nmodel4 = model4.to(device)\nmodel4.eval()\n\nmodel5 = torch.load(model_path + 'image_classification_augmentation_model5.pt')\nmodel5 = model5.to(device)\nmodel5.eval()\n\nresult1 = []\n\nprob1 = []\nprob2 = []\nprob3 = []\nprob4 = []\nprob5 = []\nprob_result = []\n\nwith torch.no_grad():\n    for id in test_df.id.values:\n        img_name = image_path + str(id) + '.jpg'\n        image = Image.open(img_name)\n\n        tensor_image = preprocess_test(image).unsqueeze(0).to(device)\n\n        pred1 = model1(tensor_image)\n        pred2 = model2(tensor_image)\n        pred3 = model3(tensor_image)\n        pred4 = model4(tensor_image)\n        pred5 = model5(tensor_image)\n\n        prob1.append(pred1[0].cpu().numpy())\n        prob2.append(pred2[0].cpu().numpy())\n        prob3.append(pred3[0].cpu().numpy())\n        prob4.append(pred4[0].cpu().numpy())\n        prob5.append(pred5[0].cpu().numpy())\n\n        prediction1 = (nn.Softmax(dim=1)(pred1) + nn.Softmax(dim=1)(pred2) + nn.Softmax(dim=1)(pred3)\n                       + nn.Softmax(dim=1)(pred4) + nn.Softmax(dim=1)(pred5))\n\n        prob_result.append(prediction1[0].cpu().numpy())\n\n        prediction1 = prediction1.argmax(dim=1).item()\n        \n        result1.append([id, categories[prediction1]])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"torch.save(torch.tensor(prob1), 'prob1_image.pt')\ntorch.save(torch.tensor(prob2), 'prob2_image.pt')\ntorch.save(torch.tensor(prob3), 'prob3_image.pt')\ntorch.save(torch.tensor(prob4), 'prob4_image.pt')\ntorch.save(torch.tensor(prob5), 'prob5_image.pt')\n\ntorch.save(torch.tensor(prob_result), 'prob_result_image.pt')\n\nheaders = ['id', 'category']\n\npredict1 = pd.DataFrame(result1, columns=headers)\n\nprint(predict1)\n\npredict1.to_csv('submission_image.csv', index=False) # accuracy 94-95%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prob1_train = []\nprob2_train = []\nprob3_train = []\nprob4_train = []\nprob5_train = []\nprob_result_train = []\n\nwith torch.no_grad():\n    for id in train_df.id.values:\n        img_name = image_path + str(id) + '.jpg'\n        image = Image.open(img_name)\n\n        tensor_image = preprocess_test(image).unsqueeze(0).to(device)\n\n        pred1 = model1(tensor_image)\n        pred2 = model2(tensor_image)\n        pred3 = model3(tensor_image)\n        pred4 = model4(tensor_image)\n        pred5 = model5(tensor_image)\n\n        prob1_train.append(pred1[0].cpu().numpy())\n        prob2_train.append(pred2[0].cpu().numpy())\n        prob3_train.append(pred3[0].cpu().numpy())\n        prob4_train.append(pred4[0].cpu().numpy())\n        prob5_train.append(pred5[0].cpu().numpy())\n\n        prediction = (nn.Softmax(dim=1)(pred1) + nn.Softmax(dim=1)(pred2) + nn.Softmax(dim=1)(pred3)\n                       + nn.Softmax(dim=1)(pred4) + nn.Softmax(dim=1)(pred5))\n\n        prob_result_train.append(prediction[0].cpu().numpy())\n\n        \ntorch.save(torch.tensor(prob1_train), 'prob1_image_train.pt')\ntorch.save(torch.tensor(prob2_train), 'prob2_image_train.pt')\ntorch.save(torch.tensor(prob3_train), 'prob3_image_train.pt')\ntorch.save(torch.tensor(prob4_train), 'prob4_image_train.pt')\ntorch.save(torch.tensor(prob5_train), 'prob5_image_train.pt')\n\ntorch.save(torch.tensor(prob_result_train), 'prob_result_image_train.pt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text"},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_graphs(history, metric):\n    plt.plot(history.history[metric])\n    plt.plot(history.history['val_'+metric], '')\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(metric)\n    plt.legend([metric, 'val_'+metric])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"text_target = train_df[['noisyTextDescription', 'category']].values\n\nprint(text_target)\n\nsplits = np.array_split(text_target, 5)\n\ntrain_data1 = np.concatenate(np.delete(splits, 4, 0))\nvalidation_data1 = splits[4]\n\ntrain_data2 = np.concatenate(np.delete(splits, 3, 0))\nvalidation_data2 = splits[3]\n\ntrain_data3 = np.concatenate(np.delete(splits, 2, 0))\nvalidation_data3 = splits[2]\n\ntrain_data4 = np.concatenate(np.delete(splits, 1, 0))\nvalidation_data4 = splits[1]\n\ntrain_data5 = np.concatenate(np.delete(splits, 0, 0))\nvalidation_data5 = splits[0]\n\n\ntrain_x1 = train_data1[:,0]\ntrain_y1 = train_data1[:,1].astype(np.int64)\nvalidation_x1 = validation_data1[:,0]\nvalidation_y1 = validation_data1[:,1].astype(np.int64)\n\ntrain_x2 = train_data2[:,0]\ntrain_y2 = train_data2[:,1].astype(np.int64)\nvalidation_x2 = validation_data2[:,0]\nvalidation_y2 = validation_data2[:,1].astype(np.int64)\n\ntrain_x3 = train_data3[:,0]\ntrain_y3 = train_data3[:,1].astype(np.int64)\nvalidation_x3 = validation_data3[:,0]\nvalidation_y3 = validation_data3[:,1].astype(np.int64)\n\ntrain_x4 = train_data4[:,0]\ntrain_y4 = train_data4[:,1].astype(np.int64)\nvalidation_x4 = validation_data4[:,0]\nvalidation_y4 = validation_data4[:,1].astype(np.int64)\n\ntrain_x5 = train_data5[:,0]\ntrain_y5 = train_data5[:,1].astype(np.int64)\nvalidation_x5 = validation_data5[:,0]\nvalidation_y5 = validation_data5[:,1].astype(np.int64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Vectorization "},{"metadata":{"trusted":true},"cell_type":"code","source":"VOCAB_SIZE= 2048\nencoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n    max_tokens=VOCAB_SIZE)\n\nencoder.adapt(np.concatenate([train_df.noisyTextDescription.values, test_df.noisyTextDescription.values]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Parameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64\nepochs = 15\n\nnum_classes=26\nassert(len(categories) == num_classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Text Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1 = tf.keras.Sequential([\n    encoder,\n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=1500,\n        mask_zero=True),\n        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(1500)),\n    tf.keras.layers.Dense(700, activation='relu'),\n    tf.keras.layers.Dense(200, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(num_classes)\n])\n\nmodel2 = tf.keras.Sequential([\n    encoder,\n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=1500,\n        mask_zero=True),\n        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(1500)),\n    tf.keras.layers.Dense(700, activation='relu'),\n    tf.keras.layers.Dense(200, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(num_classes)\n])\n\nmodel3 = tf.keras.Sequential([\n    encoder,\n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=1500,\n        mask_zero=True),\n        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(1500)),\n    tf.keras.layers.Dense(700, activation='relu'),\n    tf.keras.layers.Dense(200, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(num_classes)\n])\n\nmodel4 = tf.keras.Sequential([\n    encoder,\n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=1500,\n        mask_zero=True),\n        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(1500)),\n    tf.keras.layers.Dense(700, activation='relu'),\n    tf.keras.layers.Dense(200, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(num_classes)\n])\n\nmodel5 = tf.keras.Sequential([\n    encoder,\n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=1500,\n        mask_zero=True),\n        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(1500)),\n    tf.keras.layers.Dense(700, activation='relu'),\n    tf.keras.layers.Dense(200, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(num_classes)\n])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Callbacks"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a callback that will save the best model while training\nsave_best_model1 = ModelCheckpoint('best_text_model1', monitor='val_accuracy', mode='max', save_best_only=True, verbose=0, save_format = 'tf')\nsave_best_model2 = ModelCheckpoint('best_text_model2', monitor='val_accuracy', mode='max', save_best_only=True, verbose=0, save_format = 'tf')\nsave_best_model3 = ModelCheckpoint('best_text_model3', monitor='val_accuracy', mode='max', save_best_only=True, verbose=0, save_format = 'tf')\nsave_best_model4 = ModelCheckpoint('best_text_model4', monitor='val_accuracy', mode='max', save_best_only=True, verbose=0, save_format = 'tf')\nsave_best_model5 = ModelCheckpoint('best_text_model5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=0, save_format = 'tf')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Compile and Train"},{"metadata":{"trusted":true},"cell_type":"code","source":"model1.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(9e-5),\n              metrics=['accuracy'])\n\nmodel2.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(9e-5),\n              metrics=['accuracy'])\n\nmodel3.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(9e-5),\n              metrics=['accuracy'])\n\nmodel4.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(9e-5),\n              metrics=['accuracy'])\n\nmodel5.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              optimizer=tf.keras.optimizers.Adam(9e-5),\n              metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Fit model on training data\")\nhistory1 = model1.fit(\n    train_x1,\n    train_y1,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_data=(validation_x1, validation_y1),\n    callbacks=[save_best_model1]\n)\n\nhistory2 = model2.fit(\n    train_x2,\n    train_y2,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_data=(validation_x2, validation_y2),\n    callbacks=[save_best_model2]\n)\n\nhistory3 = model3.fit(\n    train_x3,\n    train_y3,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_data=(validation_x3, validation_y3),\n    callbacks=[save_best_model3]\n)\n\nhistory4 = model4.fit(\n    train_x4,\n    train_y4,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_data=(validation_x4, validation_y4),\n    callbacks=[save_best_model4]\n)\n\n\nhistory5 = model5.fit(\n    train_x5,\n    train_y5,\n    batch_size=batch_size,\n    epochs=epochs,\n    validation_data=(validation_x5, validation_y5),\n    callbacks=[save_best_model5]\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Plot Training Graphs"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nplot_graphs(history1, 'accuracy')\nplt.ylim(None,1)\nplt.subplot(1,2,2)\nplot_graphs(history1, 'loss')\nplt.ylim(0,None)\n\nplt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nplot_graphs(history2, 'accuracy')\nplt.ylim(None,1)\nplt.subplot(1,2,2)\nplot_graphs(history2, 'loss')\nplt.ylim(0,None)\n\nplt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nplot_graphs(history3, 'accuracy')\nplt.ylim(None,1)\nplt.subplot(1,2,2)\nplot_graphs(history3, 'loss')\nplt.ylim(0,None)\n\nplt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nplot_graphs(history4, 'accuracy')\nplt.ylim(None,1)\nplt.subplot(1,2,2)\nplot_graphs(history4, 'loss')\nplt.ylim(0,None)\n\nplt.figure(figsize=(16,8))\nplt.subplot(1,2,1)\nplot_graphs(history5, 'accuracy')\nplt.ylim(None,1)\nplt.subplot(1,2,2)\nplot_graphs(history5, 'loss')\nplt.ylim(0,None)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Save Text Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_text = test_df['noisyTextDescription'].values\n\nmodel1 = load_model('best_text_model1')\nmodel2 = load_model('best_text_model2')\nmodel3 = load_model('best_text_model3')\nmodel4 = load_model('best_text_model4')\nmodel5 = load_model('best_text_model5')\n\npred1 = model1.predict(test_text)\npred2 = model2.predict(test_text)\npred3 = model3.predict(test_text)\npred4 = model4.predict(test_text)\npred5 = model5.predict(test_text)\n\nprob1 = torch.tensor(pred1)\nprob2 = torch.tensor(pred2)\nprob3 = torch.tensor(pred3)\nprob4 = torch.tensor(pred4)\nprob5 = torch.tensor(pred5)\n\ntorch.save(prob1, 'prob1_text.pt')\ntorch.save(prob2, 'prob2_text.pt')\ntorch.save(prob3, 'prob3_text.pt')\ntorch.save(prob4, 'prob4_text.pt')\ntorch.save(prob5, 'prob5_text.pt')\n\nprob_result = (nn.Softmax(dim=1)(prob1) + nn.Softmax(dim=1)(prob2) + nn.Softmax(dim=1)(prob3)\n                       + nn.Softmax(dim=1)(prob4) + nn.Softmax(dim=1)(prob5))\n\ntorch.save(torch.tensor(prob_result), 'prob_result_text.pt')\n\nprediction1 = prob_result.argmax(dim=1)\n\ntest_df['category'] = categories[torch.flatten(prediction1).numpy()]\n\nheaders = ['id', 'category']\n\nresult = test_df[headers].values\n\npredict = pd.DataFrame(result, columns=headers)\n\nprint(predict)\n\npredict.to_csv('submission_text.csv', index=False) # accuracy 91-92%","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text = train_df['noisyTextDescription'].values\n\npred1 = model1.predict(train_text)\npred2 = model2.predict(train_text)\npred3 = model3.predict(train_text)\npred4 = model4.predict(train_text)\npred5 = model5.predict(train_text)\n\nprob1 = torch.tensor(pred1)\nprob2 = torch.tensor(pred2)\nprob3 = torch.tensor(pred3)\nprob4 = torch.tensor(pred4)\nprob5 = torch.tensor(pred5)\n\ntorch.save(prob1, 'prob1_text_train.pt')\ntorch.save(prob2, 'prob2_text_train.pt')\ntorch.save(prob3, 'prob3_text_train.pt')\ntorch.save(prob4, 'prob4_text_train.pt')\ntorch.save(prob5, 'prob5_text_train.pt')\n\nprob_result = (nn.Softmax(dim=1)(prob1) + nn.Softmax(dim=1)(prob2) + nn.Softmax(dim=1)(prob3)\n                       + nn.Softmax(dim=1)(prob4) + nn.Softmax(dim=1)(prob5))\n\ntorch.save(torch.tensor(prob_result), 'prob_result_text_train.pt')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combine Categorical Data and the Results of Image and Text"},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def to_onehot(labels, num_labels):\n    b = np.zeros((labels.size, num_labels))\n    b[np.arange(labels.size),labels] = 1\n    return b","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_genders = train_df.gender.values\ntest_genders = test_df.gender.values\n\ntrain_baseColours = train_df.baseColour.values\ntest_baseColours = test_df.baseColour.values\n\ntrain_seasons = train_df.season.values\ntest_seasons = test_df.season.values\n\ntrain_usages = train_df.usage.values\ntest_usages = test_df.usage.values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_predict = torch.load('prob_result_image_train.pt')\ntext_predict = torch.load('prob_result_text_train.pt')\ntargets = (train_df.category.values)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"indices = np.random.permutation(data_size)\n\nimage_predict = image_predict[indices]\ntext_predict = text_predict[indices]\ntargets = targets[indices]\n\ntrain_genders = to_onehot(train_genders[indices], num_genders)\ntest_genders = to_onehot(test_genders, num_genders)\n\ntrain_baseColours = to_onehot(train_baseColours[indices], num_baseColours)\ntest_baseColours = to_onehot(test_baseColours, num_baseColours)\n\ntrain_seasons = to_onehot(train_seasons[indices], num_seasons)\ntest_seasons = to_onehot(test_seasons, num_seasons)\n\ntrain_usages = to_onehot(train_usages[indices], num_usages)\ntest_usages = to_onehot(test_usages, num_usages)\n\nassert(len(train_genders) == len(train_baseColours))\n\ntrain_cat = np.hstack((train_genders, train_baseColours, train_seasons, train_usages))\n\ntrain_x = torch.cat((image_predict, text_predict), 1).numpy()\ntrain_y = targets\n\nsplits_cat = np.array_split(train_cat, 5)\nsplits_x = np.array_split(train_x, 5)\nsplits_y = np.array_split(targets, 5)\n\n\n\ntrain_data_cat1 = torch.tensor(np.concatenate(np.delete(splits_cat, 4, 0)), dtype=torch.float32)\ntrain_data_x1 = np.concatenate(np.delete(splits_x, 4, 0))\ntrain_data_y1 = np.concatenate(np.delete(splits_y, 4, 0))\n\nvalidation_data_cat1 = torch.tensor(splits_cat[4], dtype=torch.float32)\nvalidation_data_x1 = splits_x[4]\nvalidation_data_y1 = splits_y[4]\n\ntrain_data_cat2 = torch.tensor(np.concatenate(np.delete(splits_cat, 3, 0)), dtype=torch.float32)\ntrain_data_x2 = np.concatenate(np.delete(splits_x, 3, 0))\ntrain_data_y2 = np.concatenate(np.delete(splits_y, 3, 0))\n\nvalidation_data_cat2 = torch.tensor(splits_cat[3], dtype=torch.float32)\nvalidation_data_x2 = splits_x[3]\nvalidation_data_y2 = splits_y[3]\n\ntrain_data_cat3 = torch.tensor(np.concatenate(np.delete(splits_cat, 2, 0)), dtype=torch.float32)\ntrain_data_x3 = np.concatenate(np.delete(splits_x, 2, 0))\ntrain_data_y3 = np.concatenate(np.delete(splits_y, 2, 0))\n\nvalidation_data_cat3 = torch.tensor(splits_cat[2], dtype=torch.float32)\nvalidation_data_x3 = splits_x[2]\nvalidation_data_y3 = splits_y[2]\n\ntrain_data_cat4 = torch.tensor(np.concatenate(np.delete(splits_cat, 1, 0)), dtype=torch.float32)\ntrain_data_x4 = np.concatenate(np.delete(splits_x, 1, 0))\ntrain_data_y4 = np.concatenate(np.delete(splits_y, 1, 0))\n\nvalidation_data_cat4 =  torch.tensor(splits_cat[1], dtype=torch.float32)\nvalidation_data_x4 = splits_x[1]\nvalidation_data_y4 = splits_y[1]\n\ntrain_data_cat5 = torch.tensor(np.concatenate(np.delete(splits_cat, 0, 0)), dtype=torch.float32)\ntrain_data_x5 = np.concatenate(np.delete(splits_x, 0, 0))\ntrain_data_y5 = np.concatenate(np.delete(splits_y, 0, 0))\n\nvalidation_data_cat5 =  torch.tensor(splits_cat[0], dtype=torch.float32)\nvalidation_data_x5 = splits_x[0]\nvalidation_data_y5 = splits_y[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# run on GPU if possible\ncuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if cuda else \"cpu\")\n\nbatch_size = 64\nlog_interval = 100","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Cat_Dataset(Dataset):\n\n    def __init__(self, data1, data2, target):\n        self.data1 = data1\n        self.data2 = data2\n        self.target = target\n\n    def __len__(self):\n        return len(self.target)\n\n    def __getitem__(self, idx):\n        return self.data1[idx], self.data2[idx], self.target[idx]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create data loaders\nkwargs = {'num_workers': 1, 'pin_memory': True} if cuda else {}\n\ntrain1 = Cat_Dataset(train_data_cat1, train_data_x1, train_data_y1)\nvalidation1 = Cat_Dataset(validation_data_cat1, validation_data_x1, validation_data_y1)\n\ntrain2 = Cat_Dataset(train_data_cat2, train_data_x2, train_data_y2)\nvalidation2 = Cat_Dataset(validation_data_cat2, validation_data_x2, validation_data_y2)\n\ntrain3 = Cat_Dataset(train_data_cat3, train_data_x3, train_data_y3)\nvalidation3 = Cat_Dataset(validation_data_cat3, validation_data_x3, validation_data_y3)\n\ntrain4 = Cat_Dataset(train_data_cat4, train_data_x4, train_data_y4)\nvalidation4 = Cat_Dataset(validation_data_cat4, validation_data_x4, validation_data_y4)\n\ntrain5 = Cat_Dataset(train_data_cat5, train_data_x5, train_data_y5)\nvalidation5 = Cat_Dataset(validation_data_cat5, validation_data_x5, validation_data_y5)\n\n\ntrain_loader1 = DataLoader(train1, batch_size=batch_size, shuffle=True, **kwargs)\nvalidation_loader1 = DataLoader(validation1, batch_size=batch_size, shuffle=True, **kwargs)\n\ntrain_loader2 = DataLoader(train2, batch_size=batch_size, shuffle=True, **kwargs)\nvalidation_loader2 = DataLoader(validation2, batch_size=batch_size, shuffle=True, **kwargs)\n\ntrain_loader3 = DataLoader(train3, batch_size=batch_size, shuffle=True, **kwargs)\nvalidation_loader3 = DataLoader(validation3, batch_size=batch_size, shuffle=True, **kwargs)\n\ntrain_loader4 = DataLoader(train4, batch_size=batch_size, shuffle=True, **kwargs)\nvalidation_loader4 = DataLoader(validation4, batch_size=batch_size, shuffle=True, **kwargs)\n\ntrain_loader5 = DataLoader(train5, batch_size=batch_size, shuffle=True, **kwargs)\nvalidation_loader5 = DataLoader(validation5, batch_size=batch_size, shuffle=True, **kwargs)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Combine Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"input_size1 = num_genders + num_baseColours + num_seasons + num_usages\ninput_size2 = 52\nnum_classes = 26\n\nclass model_combine(nn.Module):\n    def __init__(self, input_size1, input_size2, num_classes):\n        super(model_combine, self).__init__()\n\n        interim_size = 12\n\n        self.layers1 = nn.Sequential(\n            nn.Linear(input_size1, 27),\n            nn.ReLU(),\n            nn.BatchNorm1d(27),\n            nn.Dropout(p=0.2),\n            nn.Linear(27, interim_size)\n        )\n\n        self.layers2 = nn.Sequential(\n            nn.Linear(interim_size + input_size2, 45),\n            nn.ReLU(),\n            nn.BatchNorm1d(45),\n            nn.Dropout(p=0.3),\n            nn.Linear(45, 35),\n            nn.ReLU(),\n            nn.BatchNorm1d(35),\n            nn.Dropout(p=0.2),\n            nn.Linear(35, num_classes)\n        )\n\n    def forward(self, input1, input2):\n        x = self.layers1(input1)\n        combined = torch.cat((x, input2), 1)\n        output = self.layers2(combined)\n        return output\n\nmodel1 = model_combine(input_size1, input_size2, num_classes)\nmodel2 = model_combine(input_size1, input_size2, num_classes)\nmodel3 = model_combine(input_size1, input_size2, num_classes)\nmodel4 = model_combine(input_size1, input_size2, num_classes)\nmodel5 = model_combine(input_size1, input_size2, num_classes)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loss Functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"criterion1 = nn.CrossEntropyLoss(reduction='sum')\ncriterion2 = nn.CrossEntropyLoss(reduction='sum')\ncriterion3 = nn.CrossEntropyLoss(reduction='sum')\ncriterion4 = nn.CrossEntropyLoss(reduction='sum')\ncriterion5 = nn.CrossEntropyLoss(reduction='sum')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Optimizers"},{"metadata":{"trusted":true},"cell_type":"code","source":"optimizer1 = optim.Adam(model1.parameters(), lr=8e-4)\noptimizer2 = optim.Adam(model2.parameters(), lr=8e-4)\noptimizer3 = optim.Adam(model3.parameters(), lr=8e-4)\noptimizer4 = optim.Adam(model4.parameters(), lr=8e-4)\noptimizer5 = optim.Adam(model5.parameters(), lr=8e-4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_train(epoch, model, optimizer, criterion, train_loader):\n    model.train()\n    total_loss = 0\n    for batch_idx, (data1, data2, targets) in enumerate(train_loader):\n        \n        data1 = data1.to(device)\n        data2 = data2.to(device)\n        targets = targets.to(device)\n        optimizer.zero_grad()\n        recon_batch = model(data1, data2)\n        \n        loss = criterion(recon_batch, targets)\n        loss.backward()\n        total_loss += loss.item()\n        optimizer.step()\n        if batch_idx % log_interval == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(targets), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader),\n                loss.item() / len(targets)))\n\n    average_loss = total_loss / len(train_loader.dataset)\n    print('====> Epoch: {} Average loss: {:.4f}'.format(\n          epoch, average_loss))\n    return average_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_test(epoch, model, criterion, validation_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for _, (data1, data2, targets) in enumerate(validation_loader):\n            data1 = data1.to(device)\n            data2 = data2.to(device)\n            targets = targets.to(device)\n            recon_batch = model(data1, data2)\n            test_loss += criterion(recon_batch, targets).item()\n            \n            preds = recon_batch.argmax(dim=1)#, keepdim=True)\n            correct += preds.eq(targets).sum().item()\n            \n    average_test_loss = test_loss / len(validation_loader.dataset)\n    test_accuracy = correct / len(validation_loader.dataset)\n    print('====> Validation loss: {:.4f}'.format(average_test_loss))\n    print('====> Validation accuracy: {:.2f}'.format(test_accuracy))\n    return average_test_loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Main"},{"metadata":{"trusted":true},"cell_type":"code","source":"epochs = 39","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model1"},{"metadata":{"trusted":true},"cell_type":"code","source":"average_train_losses = []\naverage_test_losses = []\n\nmodel1 = model1.to(device)\n\nfor epoch in range(1, epochs + 1):\n    average_train_loss = model_train(epoch, model1, optimizer1, criterion1, train_loader1)\n    average_train_losses.append(average_train_loss)\n    average_test_loss = model_test(epoch, model1, criterion1, validation_loader1)\n\n    # save model with best validation loss\n    if epoch == 1 or average_test_loss < min(average_test_losses):\n        torch.save(model1, 'combine_with_cat_model1.pt')\n\n    average_test_losses.append(average_test_loss)\n    \n\n# Plot Training Losses\nplt.plot(average_train_losses)\nplt.title('Train Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Train'], loc='upper right')\nplt.show()\n\n# Plot Testing Losses\nplt.plot(average_test_losses)\nplt.title('Test Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Test'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model2"},{"metadata":{"trusted":true},"cell_type":"code","source":"average_train_losses = []\naverage_test_losses = []\n\nmodel2 = model2.to(device)\n\nfor epoch in range(1, epochs + 1):\n    average_train_loss = model_train(epoch, model2, optimizer2, criterion2, train_loader2)\n    average_train_losses.append(average_train_loss)\n    average_test_loss = model_test(epoch, model2, criterion2, validation_loader2)\n\n    # save model with best validation loss\n    if epoch == 1 or average_test_loss < min(average_test_losses):\n        torch.save(model2, 'combine_with_cat_model2.pt')\n\n    average_test_losses.append(average_test_loss)\n    \n\n# Plot Training Losses\nplt.plot(average_train_losses)\nplt.title('Train Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Train'], loc='upper right')\nplt.show()\n\n# Plot Testing Losses\nplt.plot(average_test_losses)\nplt.title('Test Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Test'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model 3"},{"metadata":{"trusted":true},"cell_type":"code","source":"average_train_losses = []\naverage_test_losses = []\n\nmodel3 = model3.to(device)\n\nfor epoch in range(1, epochs + 1):\n    average_train_loss = model_train(epoch, model3, optimizer3, criterion3, train_loader3)\n    average_train_losses.append(average_train_loss)\n    average_test_loss = model_test(epoch, model3, criterion3, validation_loader3)\n\n    # save model with best validation loss\n    if epoch == 1 or average_test_loss < min(average_test_losses):\n        torch.save(model3, 'combine_with_cat_model3.pt')\n\n    average_test_losses.append(average_test_loss)\n    \n\n# Plot Training Losses\nplt.plot(average_train_losses)\nplt.title('Train Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Train'], loc='upper right')\nplt.show()\n\n# Plot Testing Losses\nplt.plot(average_test_losses)\nplt.title('Test Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Test'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model4"},{"metadata":{"trusted":true},"cell_type":"code","source":"average_train_losses = []\naverage_test_losses = []\n\nmodel4 = model4.to(device)\n\nfor epoch in range(1, epochs + 1):\n    average_train_loss = model_train(epoch, model4, optimizer4, criterion4, train_loader4)\n    average_train_losses.append(average_train_loss)\n    average_test_loss = model_test(epoch, model4, criterion4, validation_loader4)\n\n    # save model with best validation loss\n    if epoch == 1 or average_test_loss < min(average_test_losses):\n        torch.save(model4, 'combine_with_cat_model4.pt')\n\n    average_test_losses.append(average_test_loss)\n    \n\n# Plot Training Losses\nplt.plot(average_train_losses)\nplt.title('Train Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Train'], loc='upper right')\nplt.show()\n\n# Plot Testing Losses\nplt.plot(average_test_losses)\nplt.title('Test Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Test'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model5"},{"metadata":{"trusted":true},"cell_type":"code","source":"average_train_losses = []\naverage_test_losses = []\n\nmodel5 = model5.to(device)\n\nfor epoch in range(1, epochs + 1):\n    average_train_loss = model_train(epoch, model5, optimizer5, criterion5, train_loader5)\n    average_train_losses.append(average_train_loss)\n    average_test_loss = model_test(epoch, model5, criterion5, validation_loader5)\n\n    # save model with best validation loss\n    if epoch == 1 or average_test_loss < min(average_test_losses):\n        torch.save(model5, 'combine_with_cat_model5.pt')\n\n    average_test_losses.append(average_test_loss)\n    \n\n# Plot Training Losses\nplt.plot(average_train_losses)\nplt.title('Train Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Train'], loc='upper right')\nplt.show()\n\n# Plot Testing Losses\nplt.plot(average_test_losses)\nplt.title('Test Losses')\nplt.ylabel('Cross Entropy')\nplt.xlabel('Epoch #')\nplt.legend(['Test'], loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Predict Result"},{"metadata":{"trusted":true},"cell_type":"code","source":"model_path = ''\n\nmodel1 = torch.load(model_path + 'combine_with_cat_model1.pt')\nmodel1 = model1.to(device)\nmodel1.eval()\n\nmodel2 = torch.load(model_path + 'combine_with_cat_model2.pt')\nmodel2 = model2.to(device)\nmodel2.eval()\n\nmodel3 = torch.load(model_path + 'combine_with_cat_model3.pt')\nmodel3 = model3.to(device)\nmodel3.eval()\n\nmodel4 = torch.load(model_path + 'combine_with_cat_model4.pt')\nmodel4 = model4.to(device)\nmodel4.eval()\n\nmodel5 = torch.load(model_path + 'combine_with_cat_model5.pt')\nmodel5 = model5.to(device)\nmodel5.eval()\n\nimage_predict_result = torch.load('prob_result_image.pt')\ntext_predict_result = torch.load('prob_result_text.pt')\n\ntest_data = torch.cat((image_predict_result, text_predict_result), 1).to(device) #.numpy()\n\ntest_cat = torch.tensor(np.hstack((test_genders, test_baseColours, test_seasons, test_usages)), dtype=torch.float32).to(device)\n\nassert(len(test_data) == len(test_cat))\n\nwith torch.no_grad():\n\n    pred1 = model1(test_cat, test_data)\n    pred2 = model2(test_cat, test_data)\n    pred3 = model3(test_cat, test_data)\n    pred4 = model4(test_cat, test_data)\n    pred5 = model5(test_cat, test_data)\n\n    prediction = (nn.Softmax(dim=1)(pred1) + nn.Softmax(dim=1)(pred2) + nn.Softmax(dim=1)(pred3)\n                       + nn.Softmax(dim=1)(pred4) + nn.Softmax(dim=1)(pred5))\n\n    prob_result = prediction.cpu()\n\n    prediction1 = prob_result.argmax(dim=1)\n\n    test_df['category'] = categories[prediction1.numpy()]\n\n\nheaders = ['id', 'category']\n\nresult = test_df[headers].values\n\npredict = pd.DataFrame(result, columns=headers)\n\nprint(predict)\n\npredict.to_csv('submission.csv', index=False) # accuracy 97-98%","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}